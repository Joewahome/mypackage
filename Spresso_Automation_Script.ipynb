{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPz0+9M3Xt5/rO5pM6es/dS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Joewahome/mypackage/blob/master/Spresso_Automation_Script.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGsCKC8MHx31"
      },
      "outputs": [],
      "source": [
        "from constants import (\n",
        "    post_to_slack,\n",
        "    file_to_slack,\n",
        "    dev_redshift,\n",
        "    wallet_redshift,\n",
        "    redshift,\n",
        "    insert_to_postgres,\n",
        "    bulk_insert,\n",
        "    delete_to_postgres,\n",
        "    get_all_files,\n",
        "    slack_texttable,\n",
        ")\n",
        "from common.aws.adapters import SecretsManagerAdapter\n",
        "from datetime import datetime, timedelta, date\n",
        "import pandas as pd\n",
        "from googleapiclient.discovery import build\n",
        "from google.oauth2.service_account import Credentials\n",
        "from google.auth.transport.requests import Request\n",
        "import gspread\n",
        "import numpy as np\n",
        "import logging\n",
        "import io\n",
        "import pytz\n",
        "\n",
        "eastern = pytz.timezone(\"Asia/Dubai\")\n",
        "\n",
        "\n",
        "def get_location(file_name):\n",
        "    if \"66\" in file_name:\n",
        "        return \"66\"\n",
        "    elif \"67\" in file_name:\n",
        "        return \"67\"\n",
        "    elif \"68\" in file_name:\n",
        "        return \"68\"\n",
        "    elif \"69\" in file_name:\n",
        "        return \"69\"\n",
        "    elif \"70\" in file_name:\n",
        "        return \"70\"\n",
        "    elif \"80\" in file_name:\n",
        "        return \"80\"\n",
        "    elif \"81\" in file_name:\n",
        "        return \"81\"\n",
        "    elif \"82\" in file_name:\n",
        "        return \"82\"\n",
        "    elif \"83\" in file_name:\n",
        "        return \"83\"\n",
        "    elif \"84\" in file_name:\n",
        "        return \"84\"\n",
        "    elif \"85\" in file_name:\n",
        "        return \"85\"\n",
        "    elif \"86\" in file_name:\n",
        "        return \"86\"\n",
        "    elif \"87\" in file_name:\n",
        "        return \"87\"\n",
        "    elif \"88\" in file_name:\n",
        "        return \"88\"\n",
        "    elif \"89\" in file_name:\n",
        "        return \"89\"\n",
        "    elif \"90\" in file_name:\n",
        "        return \"90\"\n",
        "    elif \"91\" in file_name:\n",
        "        return \"91\"\n",
        "    elif \"92\" in file_name:\n",
        "        return \"92\"\n",
        "    elif \"93\" in file_name:\n",
        "        return \"93\"\n",
        "    elif \"94\" in file_name:\n",
        "        return \"94\"\n",
        "    else:\n",
        "        return \"other\"\n",
        "\n",
        "\n",
        "json_creds = SecretsManagerAdapter(\"da/recon/google/service-account-creds\").value\n",
        "creds = Credentials.from_service_account_info(\n",
        "    json_creds,\n",
        "    scopes=[\n",
        "        \"https://www.googleapis.com/auth/cloud-platform\",\n",
        "        \"https://www.googleapis.com/auth/drive\",\n",
        "        \"https://www.googleapis.com/auth/spreadsheets\",\n",
        "        \"https://spreadsheets.google.com/feeds\",\n",
        "        \"https://www.googleapis.com/auth/drive.file\",\n",
        "    ],\n",
        ")\n",
        "drive_service = build(\"drive\", \"v3\", credentials=creds)\n",
        "gsheet_service = gspread.authorize(creds)\n",
        "if not creds.valid:\n",
        "    creds.refresh(Request())\n",
        "\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "slack_channel = \"nigeria_reports\"\n",
        "na = \"\"\n",
        "tolerance = 1\n",
        "neg_tolerance = -1\n",
        "\n",
        "\n",
        "def generate_reconciliation_handler(event, context):\n",
        "    date_str = event.get(\"date\", None)\n",
        "    seven_days_ago = date.today() - timedelta(days=7)\n",
        "    if date_str:\n",
        "        try:\n",
        "            yesterday = pd.to_datetime(date_str, format=\"%Y-%m-%d\").date()\n",
        "            if yesterday < seven_days_ago:\n",
        "                return {\n",
        "                    \"statusCode\": 400,\n",
        "                    \"body\": \"Invalid date input. Date should be within the last 7 days.\",\n",
        "                }\n",
        "        except ValueError:\n",
        "            return {\n",
        "                \"statusCode\": 400,\n",
        "                \"body\": f\"Invalid date format: {date_str}. Expected format is yyyy-mm-dd.\",\n",
        "            }\n",
        "    else:\n",
        "        yesterday = date.today() - timedelta(days=1)\n",
        "    yesterday_date = pd.to_datetime(yesterday, format=\"%Y-%m-%d\")\n",
        "    smart_assignment = yesterday_date + pd.Timedelta(days=1)\n",
        "    yesterday_date = yesterday_date.strftime(\"%Y-%m-%d\")\n",
        "    smart_assignment_date = smart_assignment.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    logger.info(\"Generating new spresso recon\")\n",
        "    logger.info(\"dropping recon records\")\n",
        "    delete_to_postgres(\"nigeria.reconciliation_spresso\", {\"day\": yesterday_date})\n",
        "    logger.info(\"dropping payout records\")\n",
        "    delete_to_postgres(\"nigeria.payout\", {\"reconciliation_date\": yesterday_date})\n",
        "\n",
        "    details = f\"S-Presso Daily recon {yesterday} script starts: {datetime.now(eastern).strftime('%Y-%m-%d %H:%M:%S')}\"\n",
        "    post_to_slack(details=details, channel=slack_channel)\n",
        "\n",
        "    logger.info(f\"Fetching active drivers (smart assignment)\")\n",
        "    active_drivers = dev_redshift(\n",
        "        f\"\"\"select drn,pick_drop_date\n",
        "    from (select\n",
        "        row_number() over (partition by drn\n",
        "    order by\n",
        "        effective_end_date desc) as rnk,\n",
        "        case when cast((effective_start_date AT TIME ZONE 'UTC' AT TIME ZONE 'Africa/Lagos') as date) = cast('{yesterday_date}' as date) or\n",
        "        cast((effective_end_date AT TIME ZONE 'UTC' AT TIME ZONE 'Africa/Lagos') as DATE) = cast('{yesterday_date}' as date) then 1 else 0 end as pick_drop_date,\n",
        "        *\n",
        "    from\n",
        "        (\n",
        "        select\n",
        "            c.registration_no as plate_number,\n",
        "            b.drn,\n",
        "            b.name,\n",
        "            cast((effective_start_date AT TIME ZONE 'UTC' AT TIME ZONE 'Africa/Lagos') as date) as effective_start_date,\n",
        "            coalesce(cast((effective_end_date AT TIME ZONE 'UTC' AT TIME ZONE 'Africa/Lagos') as DATE),date'2050-01-01') as effective_end_date,\n",
        "            c.make,\n",
        "            c.model,\n",
        "            c.year,\n",
        "            c.color,\n",
        "            mc.name as city,\n",
        "            c.vin,\n",
        "            a.description\n",
        "        from\n",
        "            moovebackend.driver_drivervehicleassignment a,\n",
        "            moovebackend.driver_driver b,\n",
        "            moovebackend.driver_vehicle c,\n",
        "            moovebackend.markets_city mc\n",
        "        where\n",
        "            a.driver_id = b.id\n",
        "            and a.vehicle_id = c.id\n",
        "            and c.city_id = mc.id\n",
        "        order by\n",
        "            mc.name,\n",
        "            effective_end_date desc,\n",
        "            effective_start_date desc) vehicleAssign\n",
        "    where\n",
        "        city = 'Lagos'\n",
        "        and cast((effective_start_date AT TIME ZONE 'UTC' AT TIME ZONE 'Africa/Lagos') as date) <= '{yesterday_date}'\n",
        "        and cast((effective_end_date AT TIME ZONE 'UTC' AT TIME ZONE 'Africa/Lagos') as DATE) >= '{yesterday_date}') as a\n",
        "        where rnk = 1\n",
        "    \"\"\"\n",
        "    )\n",
        "\n",
        "    dd = redshift(\n",
        "        f\"\"\"select drn,driver_name as uber_name,driver_uuid as uber_id,product,vehicle_type from (\n",
        "    select *,row_number() over (partition by drn order by date asc,product_start_date desc ) as rnk\n",
        "    from vehicle.data_dump where date >= '{smart_assignment_date}' and city = 'Lagos' and driver_uuid is not null and driver_uuid != '' and LENGTH(driver_uuid) = 36) as a\n",
        "    where rnk = 1\"\"\"\n",
        "    )\n",
        "\n",
        "    spresso_recon = pd.merge(active_drivers, dd, how=\"left\", on=\"drn\")\n",
        "    spresso_recon[\"day\"] = pd.to_datetime(yesterday)\n",
        "    spresso_recon[\"week\"] = spresso_recon[\"day\"] - pd.to_timedelta(\n",
        "        spresso_recon[\"day\"].dt.dayofweek, unit=\"d\"\n",
        "    )\n",
        "    spresso_recon = spresso_recon[\n",
        "        spresso_recon[\"product\"] == \"UberGo DTO 48 [S-Presso]\"\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        yesterday_uber = datetime.strptime(yesterday, \"%Y-%m-%d\")\n",
        "    except Exception as e:\n",
        "        yesterday_uber = yesterday\n",
        "    yesterday_string = yesterday_uber.strftime(\"%b %d\")\n",
        "    product = f\"(name contains '{yesterday_string}(66)' or name contains '{yesterday_string}(67)' or name contains '{yesterday_string}(68)' or name contains '{yesterday_string}(69)' or name contains '{yesterday_string}(70)' or name contains '{yesterday_string}(80)'or name contains '{yesterday_string}(81)'or name contains '{yesterday_string}(82)'or name contains '{yesterday_string}(83)'or name contains '{yesterday_string}(84)'or name contains '{yesterday_string}(85)' or name contains '{yesterday_string}(86)'or name contains '{yesterday_string}(87)'or name contains '{yesterday_string}(88)'or name contains '{yesterday_string}(89)'or name contains '{yesterday_string}(90)'or name contains '{yesterday_string}(91)'or name contains '{yesterday_string}(92)'or name contains '{yesterday_string}(93)'or name contains '{yesterday_string}(94)')\"\n",
        "    product_list = [\n",
        "        66,\n",
        "        67,\n",
        "        68,\n",
        "        69,\n",
        "        70,\n",
        "        80,\n",
        "        81,\n",
        "        82,\n",
        "        83,\n",
        "        84,\n",
        "        85,\n",
        "        86,\n",
        "        87,\n",
        "        88,\n",
        "        89,\n",
        "        90,\n",
        "        91,\n",
        "        92,\n",
        "        93,\n",
        "        94,\n",
        "    ]\n",
        "    weekFolderID = \"1BcYdBsK5UQYD6YUD5eOo5-8w_LP3D86l\"\n",
        "    uberGo48SP = \"(title contains '(66)' or title contains '(67)' or title contains '(68)' or title contains '(69)' or title contains '(70)' or title contains '(80)'or title contains '(81)'or title contains '(82)'or title contains '(83)'or title contains '(84)'or title contains '(85)' or title contains '(86)'or title contains '(87)'or title contains '(88)'or title contains '(89)'or title contains '(90)'or title contains '(91)'or title contains '(92)'or title contains '(93)'or title contains '(94)')\"\n",
        "    query = (\n",
        "        \"'\"\n",
        "        + weekFolderID\n",
        "        + \"' in parents and  mimeType = 'application/vnd.google-apps.folder'\"\n",
        "    )\n",
        "    weekFolder = (\n",
        "        drive_service.files()\n",
        "        .list(q=query, fields=\"nextPageToken, files(id, name)\")\n",
        "        .execute()\n",
        "    )\n",
        "    weekFolder = weekFolder.get(\"files\", [])\n",
        "\n",
        "    logger.info(f\"Fetching payments uber files\")\n",
        "    file_list = []\n",
        "\n",
        "    def combine_payments_files(country):\n",
        "        listframe = []\n",
        "        for folder in weekFolder:\n",
        "            if folder[\"name\"] == \"Payments\":\n",
        "                query = f\"mimeType='text/csv' and trashed=false and {product} and parents in '{folder['id']}' \"\n",
        "                results = (\n",
        "                    drive_service.files()\n",
        "                    .list(q=query, fields=\"nextPageToken, files(id, name)\")\n",
        "                    .execute()\n",
        "                )\n",
        "                files = results.get(\"files\", [])\n",
        "                for file in files:\n",
        "                    mk1 = file[\"name\"].find(\"(\") + 1\n",
        "                    mk2 = file[\"name\"].find(\")\", mk1)\n",
        "                    subString = file[\"name\"][mk1:mk2]\n",
        "                    if int(subString) in product_list:\n",
        "                        file_list.append(file[\"name\"])\n",
        "                        request = drive_service.files().get_media(fileId=file[\"id\"])\n",
        "                        file_content = request.execute()\n",
        "                        df = pd.read_csv(\n",
        "                            io.StringIO(file_content.decode(\"utf-8\")), delimiter=\",\"\n",
        "                        )\n",
        "                        df = df.rename(\n",
        "                            {\n",
        "                                \"Paid to you : Your earnings\": \"Net Earnings\",\n",
        "                                \"Paid to you : Trip balance : Payouts : Cash collected\": \"Cash Collected\",\n",
        "                                \"Paid to you\": \"Uber Balance\",\n",
        "                                \"Paid to you : Trip balance : Payouts : Cash Collected\": \"Cash Collected\",\n",
        "                                \"Paid to you : Your earnings : Fare\": \"Net Fares\",\n",
        "                                \"Paid to you:Your earnings:Service fee\": \"Uber Fee\",\n",
        "                                \"Paid to you:Your earnings:Service Fee\": \"Uber Fee\",\n",
        "                            },\n",
        "                            axis=1,\n",
        "                        )\n",
        "                        df[\"name\"] = file[\"name\"]\n",
        "                        listframe.append(df)\n",
        "                        row_count = len(df)\n",
        "                        print(f\"{file['name']}: {row_count} rows\")\n",
        "                        if len(df) < 3:\n",
        "                            post_to_slack(\n",
        "                                details=f\"Payment file alert - one file had no data {file['name']}\",\n",
        "                                channel=slack_channel,\n",
        "                                title=\"Uber files\",\n",
        "                            )\n",
        "        daypaymentsfile = pd.concat(listframe)\n",
        "        words_to_match = (\n",
        "            \"Diamond Challenge\",\n",
        "            \"Quest\",\n",
        "            \"Guarantee Earnings\",\n",
        "            \"guarantee\",\n",
        "        )\n",
        "        daypaymentsfile[\"Uber Incentives\"] = daypaymentsfile[\"Net Earnings\"].where(\n",
        "            daypaymentsfile[\"Description\"].str.contains(\n",
        "                \"|\".join(words_to_match), case=False, na=False\n",
        "            ),\n",
        "            0,\n",
        "        )\n",
        "        daypaymentsfile = daypaymentsfile[\n",
        "            daypaymentsfile[\"Driver UUID\"] != \"00000000-0000-0000-0000-000000000000\"\n",
        "        ]\n",
        "        daypaymentsfile = (\n",
        "            daypaymentsfile.groupby([\"Driver UUID\", \"name\"])\n",
        "            .agg(\n",
        "                {\n",
        "                    \"Net Earnings\": \"sum\",\n",
        "                    \"Cash Collected\": [\"sum\", lambda x: (x < 0).sum()],\n",
        "                    \"Uber Balance\": \"sum\",\n",
        "                    \"Uber Fee\": \"sum\",\n",
        "                    \"Uber Incentives\": \"sum\",\n",
        "                }\n",
        "            )\n",
        "            .reset_index()\n",
        "        )\n",
        "        daypaymentsfile.columns = [\n",
        "            \"Driver UUID\",\n",
        "            \"name\",\n",
        "            \"Net Earnings\",\n",
        "            \"Cash Collected\",\n",
        "            \"CashTrips\",\n",
        "            \"Uber Balance\",\n",
        "            \"Uber Fee\",\n",
        "            \"Uber Incentives\",\n",
        "        ]\n",
        "        return daypaymentsfile\n",
        "\n",
        "    try:\n",
        "        payment_file = combine_payments_files(uberGo48SP)\n",
        "        payment_file_col = {\n",
        "            \"Driver UUID\": \"uber_id\",\n",
        "            \"Net Earnings\": \"net_earnings\",\n",
        "            \"Cash Collected\": \"cash_collected\",\n",
        "            \"Uber Balance\": \"uber_balance\",\n",
        "            \"Uber Incentives\": \"uber_incentive\",\n",
        "        }\n",
        "        payment_file = payment_file.rename(columns=payment_file_col)\n",
        "        id_with_file = payment_file[[\"uber_id\", \"name\"]]\n",
        "        payment_file = payment_file.drop(columns=[\"CashTrips\", \"Uber Fee\", \"name\"])\n",
        "    except Exception as e:\n",
        "        post_to_slack(\n",
        "            details=f\"SPresso Payments files failed, reason: {str(e)}\",\n",
        "            channel=slack_channel,\n",
        "            title=\"Performance files\",\n",
        "        )\n",
        "\n",
        "    not_list = id_with_file[~id_with_file[\"uber_id\"].isin(spresso_recon[\"uber_id\"])]\n",
        "    if len(not_list) > 0:\n",
        "        logger.info(f\"Adding missing uber id found in payments file\")\n",
        "        not_list[\"uber_id\"] = not_list[\"uber_id\"].str.strip()\n",
        "        uber_id_list = \"','\".join(not_list[\"uber_id\"])\n",
        "        missing_drn = redshift(\n",
        "            f\"\"\"select cast('{yesterday}' as date) AS day,\n",
        "       drn,driver_name as uber_name,driver_uuid as uber_id,product,vehicle_type,0 as pick_drop_date\n",
        "       from (select ROW_NUMBER() OVER (PARTITION BY drn ORDER BY product_start_date desc, date asc) as rnk,*\n",
        "       from vehicle.data_dump dd where date >= '{smart_assignment_date}' and product = 'UberGo DTO 48 [S-Presso]'\n",
        "       and LENGTH(driver_uuid) = 36\n",
        "       and driver_uuid in ('{uber_id_list}')) as a where rnk = 1 \"\"\"\n",
        "        )\n",
        "        spresso_recon = pd.concat([spresso_recon, missing_drn])\n",
        "        spresso_recon[\"day\"] = pd.to_datetime(spresso_recon[\"day\"])\n",
        "        spresso_recon[\"week\"] = spresso_recon[\"day\"] - pd.to_timedelta(\n",
        "            spresso_recon[\"day\"].dt.dayofweek, unit=\"d\"\n",
        "        )\n",
        "        no_record = not_list[~not_list[\"uber_id\"].isin(missing_drn[\"uber_id\"])]\n",
        "\n",
        "    logger.info(\"Fetching Payments\")\n",
        "    wallet_upi = wallet_redshift(\n",
        "        f\"\"\"select transaction_date - interval '1' day as day,\n",
        "    driver_number as drn,\n",
        "    concat('#',payment_unique_reference) as reference_number,\n",
        "    amount as bank_statement,\n",
        "    coalesce(metadata->>'remittance_type','Unknown')  AS remittance_type\n",
        "     from public.aggregated_transactions\n",
        "    where country_code = 'NG'\n",
        "    and transaction_date = '{smart_assignment_date}'\n",
        "    \"\"\"\n",
        "    )\n",
        "\n",
        "    logger.info(\"Fetching Missing Payments\")\n",
        "    missing_upi = redshift(\n",
        "        f\"\"\"select transaction_date - interval '1' day as day,\n",
        "    drn,\n",
        "    concat('#',reference_number) as reference_number,\n",
        "    amount as bank_statement,\n",
        "    'missing' as remittance_type\n",
        "    from nigeria.missing_payments\n",
        "    where transaction_date = '{smart_assignment_date}'\n",
        "    \"\"\"\n",
        "    )\n",
        "    current_upi = pd.concat([wallet_upi, missing_upi])\n",
        "\n",
        "    logger.info(\"Fetching Security to remittance payments\")\n",
        "    sec_to_rem_list = gsheet_service.open_by_url(\n",
        "        \"https://docs.google.com/spreadsheets/d/1nnpCEB-FLbx7roSqFPbQzlv3Gt2sJjVR8dq23AfqYOM\"\n",
        "    )\n",
        "    sec_to_rem_list = sec_to_rem_list.worksheet(\"Sec-Dep to Remittance\")\n",
        "    attempts = 0\n",
        "    while attempts < 10:\n",
        "        try:\n",
        "            sec_to_rem_list = pd.DataFrame(sec_to_rem_list.get_all_records())\n",
        "            break\n",
        "        except Exception as e:\n",
        "            attempts += 1\n",
        "            if attempts == 10:\n",
        "                post_to_slack(\n",
        "                    details=f\"Nigeria Sec-Dep read sheet failed, reason: {str(e)}\",\n",
        "                    channel=slack_channel,\n",
        "                )\n",
        "                raise Exception(\"Operation failed after 10 attempts.\")\n",
        "    sec_to_rem_list = current_upi[\n",
        "        current_upi[\"reference_number\"].isin(sec_to_rem_list[\"script_reference\"])\n",
        "    ]\n",
        "\n",
        "    logger.info(\"Fetching Remittance to Security payments\")\n",
        "    rem_to_sec_list = gsheet_service.open_by_url(\n",
        "        \"https://docs.google.com/spreadsheets/d/1nnpCEB-FLbx7roSqFPbQzlv3Gt2sJjVR8dq23AfqYOM\"\n",
        "    )\n",
        "    rem_to_sec_list = rem_to_sec_list.worksheet(\"Remittance to Sec-Dep\")\n",
        "    attempts = 0\n",
        "    while attempts < 10:\n",
        "        try:\n",
        "            rem_to_sec_list = pd.DataFrame(rem_to_sec_list.get_all_records())\n",
        "            break\n",
        "        except Exception as e:\n",
        "            attempts += 1\n",
        "            if attempts == 10:\n",
        "                post_to_slack(\n",
        "                    details=f\"Nigeria Sec-Dep read sheet failed, reason: {str(e)}\",\n",
        "                    channel=slack_channel,\n",
        "                )\n",
        "                raise Exception(\"Operation failed after 10 attempts.\")\n",
        "    rem_to_sec_list = current_upi[\n",
        "        current_upi[\"reference_number\"].isin(rem_to_sec_list[\"script_reference\"])\n",
        "    ]\n",
        "\n",
        "    logger.info(\"Fetching Final Payments List\")\n",
        "    current_upi = current_upi[current_upi[\"remittance_type\"] != \"SECURITY_DEPOSIT\"]\n",
        "    current_upi = pd.concat([current_upi, sec_to_rem_list])\n",
        "    current_upi = current_upi[\n",
        "        ~current_upi[\"reference_number\"].isin(rem_to_sec_list[\"reference_number\"])\n",
        "    ]\n",
        "    current_upi = current_upi.drop_duplicates(subset=\"reference_number\", keep=\"first\")\n",
        "    current_upi = (\n",
        "        current_upi.groupby([\"day\", \"drn\"])[\"bank_statement\"].sum().reset_index()\n",
        "    )\n",
        "\n",
        "    logger.info(\"Adding unassigned drivers make a payment\")\n",
        "    if len(current_upi) > 0:\n",
        "        dd = redshift(\n",
        "            f\"\"\"select drn,driver_name as uber_name,driver_uuid as uber_id,product,vehicle_type from\n",
        "      (select row_number() over (partition by drn order by date asc,product_start_date desc ) as rnk,*\n",
        "      from vehicle.data_dump where date >= '{smart_assignment_date}'\n",
        "      and LENGTH(driver_uuid) = 36 and product = 'UberGo DTO 48 [S-Presso]') dd\n",
        "      where rnk = 1 \"\"\"\n",
        "        )\n",
        "        merge_upi = pd.merge(current_upi, dd, how=\"left\", on=\"drn\")\n",
        "        spresso_upi = merge_upi[~merge_upi[\"uber_id\"].isna()]\n",
        "        if len(spresso_upi) > 0:\n",
        "            cols = [\"drn\", \"bank_statement\"]\n",
        "            active_spresso_upi = spresso_upi[cols]\n",
        "            spresso_recon = pd.merge(\n",
        "                spresso_recon, active_spresso_upi, how=\"left\", on=\"drn\"\n",
        "            )\n",
        "            spresso_recon[\"bank_statement\"].fillna(0, inplace=True)\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "        inactive_drn = spresso_upi[~spresso_upi[\"drn\"].isin(spresso_recon[\"drn\"])]\n",
        "        if len(inactive_drn) > 0:\n",
        "            cols = [\"day\", \"drn\", \"bank_statement\", \"uber_name\", \"uber_id\"]\n",
        "            trnxs_require_input = inactive_drn[cols]\n",
        "            # storing inactive drivers in gsheet for ops team\n",
        "            sh = gsheet_service.open_by_key(\n",
        "                \"1Z28QYhmQv2aAFiwBHq34v4HksqJE2TXaRqRIUwG8JWk\"\n",
        "            ).worksheet(\"payments_inactive_drn\")\n",
        "            all_values = sh.get_all_values()\n",
        "            last_row = len([i for i in all_values if i != []])\n",
        "            trnxs_require_input[\"day\"] = trnxs_require_input[\"day\"].apply(\n",
        "                lambda x: x.strftime(\"%Y-%m-%d\") if not pd.isna(x) else \"\"\n",
        "            )\n",
        "            trnxs_require_input = trnxs_require_input.replace({np.nan: \"\"})\n",
        "            sh.append_rows(trnxs_require_input.values.tolist())\n",
        "            post_to_slack(\n",
        "                details=f\"Payment uploaded to tab payments_inactive_drn\",\n",
        "                channel=slack_channel,\n",
        "            )\n",
        "            # setting fixed pick_drop_date to 2 (for inactive drivers - payment)\n",
        "            inactive_drn[\"pick_drop_date\"] = 2\n",
        "            inactive_drn[\"week\"] = inactive_drn[\"day\"] - pd.to_timedelta(\n",
        "                inactive_drn[\"day\"].dt.dayofweek, unit=\"d\"\n",
        "            )\n",
        "            cols = [\n",
        "                \"day\",\n",
        "                \"drn\",\n",
        "                \"uber_name\",\n",
        "                \"uber_id\",\n",
        "                \"product\",\n",
        "                \"vehicle_type\",\n",
        "                \"pick_drop_date\",\n",
        "                \"week\",\n",
        "                \"bank_statement\",\n",
        "            ]\n",
        "            inactive_drn = inactive_drn[cols]\n",
        "            spresso_recon = pd.concat([spresso_recon, inactive_drn])\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "    spresso_recon = pd.merge(spresso_recon, payment_file, how=\"left\", on=\"uber_id\")\n",
        "\n",
        "    logger.info(f\"Fetching performance uber files\")\n",
        "    file_list = []\n",
        "\n",
        "    def combine_performance_files(country):\n",
        "        listframe = []\n",
        "        for folder in weekFolder:\n",
        "            if folder[\"name\"] == \"Performance\":\n",
        "                query = f\"mimeType='text/csv' and trashed=false and {product} and parents in '{folder['id']}' \"\n",
        "                results = (\n",
        "                    drive_service.files()\n",
        "                    .list(q=query, fields=\"nextPageToken, files(id, name)\")\n",
        "                    .execute()\n",
        "                )\n",
        "                files = results.get(\"files\", [])\n",
        "                for file in files:\n",
        "                    mk1 = file[\"name\"].find(\"(\") + 1\n",
        "                    mk2 = file[\"name\"].find(\")\", mk1)\n",
        "                    subString = file[\"name\"][mk1:mk2]\n",
        "                    if int(subString) in product_list:\n",
        "                        file_list.append(file[\"name\"])\n",
        "                        request = drive_service.files().get_media(fileId=file[\"id\"])\n",
        "                        file_content = request.execute()\n",
        "                        df = pd.read_csv(\n",
        "                            io.StringIO(file_content.decode(\"utf-8\")), delimiter=\",\"\n",
        "                        )\n",
        "                        df = df.rename(\n",
        "                            {\n",
        "                                \"Time on trip (days: hours : minutes)\": \"Time on trip (days : hours : minutes)\",\n",
        "                                \"Driver last name\": \"Driver surname\",\n",
        "                            },\n",
        "                            axis=1,\n",
        "                        )\n",
        "                        listframe.append(df)\n",
        "                        row_count = len(df)\n",
        "                        print(f\"{file['name']}: {row_count} rows\")\n",
        "                        if len(df) < 3:\n",
        "                            post_to_slack(\n",
        "                                details=f\"Performance file alert - one file had no data {file['name']}\",\n",
        "                                channel=slack_channel,\n",
        "                                title=\"Uber files\",\n",
        "                            )\n",
        "                    else:\n",
        "                        pass\n",
        "\n",
        "        dayperffile = pd.concat(listframe)\n",
        "        dayperffile[\"Time online (days : hours: minutes)\"] = (\n",
        "            dayperffile[\"Time online (days : hours: minutes)\"]\n",
        "            .str.split(\":\")\n",
        "            .apply(lambda x: int(x[0]) * 86400 + int(x[1]) * 3600 + int(x[2]) * 60)\n",
        "        )\n",
        "        dayperffile[\"Time online (days : hours: minutes)\"] = round(\n",
        "            pd.to_timedelta(\n",
        "                dayperffile[\"Time online (days : hours: minutes)\"], unit=\"s\"\n",
        "            ).dt.total_seconds()\n",
        "            / 3600,\n",
        "            2,\n",
        "        )\n",
        "        columnslist = [\n",
        "            \"Trips completed\",\n",
        "            \"Time online (days : hours: minutes)\",\n",
        "            \"Time on trip (days : hours : minutes)\",\n",
        "        ]\n",
        "        for a in columnslist:\n",
        "            dayperffile[a] = pd.to_numeric(dayperffile[a], errors=\"coerce\")\n",
        "            dayperffile = dayperffile.rename(\n",
        "                {\"Trips completed\": \"PerfTrips completed\"}, axis=1\n",
        "            )\n",
        "            dayperffile[\"Name\"] = (\n",
        "                dayperffile[\"Driver first name\"] + \" \" + dayperffile[\"Driver surname\"]\n",
        "            )\n",
        "            dayperffile = (\n",
        "                dayperffile.groupby([\"Driver UUID\", \"Name\"])\n",
        "                .agg(\n",
        "                    {\n",
        "                        \"PerfTrips completed\": \"sum\",\n",
        "                        \"Time online (days : hours: minutes)\": \"sum\",\n",
        "                        \"Time on trip (days : hours : minutes)\": \"sum\",\n",
        "                    }\n",
        "                )\n",
        "                .reset_index()\n",
        "            )\n",
        "            return dayperffile\n",
        "\n",
        "    # perf_df = combine_performance_files(ghana_files_pattern)\n",
        "    try:\n",
        "        perf_df = combine_performance_files(uberGo48SP)\n",
        "        perf_df_col = {\n",
        "            \"Driver UUID\": \"uber_id\",\n",
        "            \"Name\": \"driver_name_perf\",\n",
        "            \"PerfTrips completed\": \"trips\",\n",
        "            \"Time online (days : hours: minutes)\": \"supply_hours\",\n",
        "            \"Time on trip (days : hours : minutes)\": \"col_to_drop\",\n",
        "        }\n",
        "        perf_df = perf_df.rename(columns=perf_df_col)\n",
        "    except Exception as e:\n",
        "        post_to_slack(\n",
        "            details=f\"S-Presso Performance files failed, reason: {str(e)}\",\n",
        "            channel=slack_channel,\n",
        "            title=\"Performance files\",\n",
        "        )\n",
        "\n",
        "    spresso_recon = pd.merge(spresso_recon, perf_df, how=\"left\", on=\"uber_id\")\n",
        "\n",
        "    logger.info(f\"Fetching quality uber files\")\n",
        "    file_list = []\n",
        "\n",
        "    def combine_driverquality_files(country):\n",
        "        listframe = []\n",
        "        for folder in weekFolder:\n",
        "            if folder[\"name\"] == \"Driver Quality\":\n",
        "                query = f\"mimeType='text/csv' and trashed=false and {product} and parents in '{folder['id']}' \"\n",
        "                results = (\n",
        "                    drive_service.files()\n",
        "                    .list(q=query, fields=\"nextPageToken, files(id, name)\")\n",
        "                    .execute()\n",
        "                )\n",
        "                files = results.get(\"files\", [])\n",
        "                for file in files:\n",
        "                    mk1 = file[\"name\"].find(\"(\") + 1\n",
        "                    mk2 = file[\"name\"].find(\")\", mk1)\n",
        "                    subString = file[\"name\"][mk1:mk2]\n",
        "                    if int(subString) in product_list:\n",
        "                        file_list.append(file[\"name\"])\n",
        "                        request = drive_service.files().get_media(fileId=file[\"id\"])\n",
        "                        file_content = request.execute()\n",
        "                        df = pd.read_csv(\n",
        "                            io.StringIO(file_content.decode(\"utf-8\")), delimiter=\",\"\n",
        "                        )\n",
        "                        df = df.rename(\n",
        "                            {\"Cancellation rate\": \"cancellation_rate\"}, axis=1\n",
        "                        )\n",
        "                        listframe.append(df)\n",
        "                        row_count = len(df)\n",
        "                        print(f\"{file['name']}: {row_count} rows\")\n",
        "                        if len(df) < 3:\n",
        "                            post_to_slack(\n",
        "                                details=f\"Driver Quality file alert - one file had no data {file['name']}\",\n",
        "                                channel=slack_channel,\n",
        "                                title=\"Uber files\",\n",
        "                            )\n",
        "        daydrvqualityfile = pd.concat(listframe)\n",
        "        cancellationrate = (\n",
        "            daydrvqualityfile[\"cancellation_rate\"]\n",
        "            .astype(str)\n",
        "            .str.split(\" \", n=0, expand=True)\n",
        "        )\n",
        "        daydrvqualityfile[\"cancellation_rate\"] = cancellationrate[0].fillna(0)\n",
        "        daydrvqualityfile[\"cancellation_rate\"] = pd.to_numeric(\n",
        "            daydrvqualityfile[\"cancellation_rate\"], errors=\"coerce\"\n",
        "        )\n",
        "        daydrvqualityfile = (\n",
        "            daydrvqualityfile.groupby([\"Driver UUID\"])\n",
        "            .agg({\"cancellation_rate\": \"mean\"})\n",
        "            .reset_index()\n",
        "        )\n",
        "        return daydrvqualityfile\n",
        "\n",
        "    try:\n",
        "        quality_file = combine_driverquality_files(uberGo48SP)\n",
        "        quality_file_col = {\"Driver UUID\": \"uber_id\"}\n",
        "        quality_file = quality_file.rename(columns=quality_file_col)\n",
        "    except Exception as e:\n",
        "        post_to_slack(\n",
        "            details=f\"SPresso Driver Quality files failed, reason: {str(e)}\",\n",
        "            channel=slack_channel,\n",
        "            title=\"Performance files\",\n",
        "        )\n",
        "\n",
        "    spresso_recon = pd.merge(spresso_recon, quality_file, how=\"left\", on=\"uber_id\")\n",
        "    spresso_recon.fillna(0, inplace=True)\n",
        "\n",
        "    logger.info(\"Fetching days off\")\n",
        "    recon_days_off = redshift(\n",
        "        f\"\"\"select drn,reconciliation_date as day,count(drn) as current_day_off from nigeria.days_off\n",
        "    where reconciliation_date = '{yesterday}' and product = 's-presso'\n",
        "    group by 1,2\"\"\"\n",
        "    )\n",
        "    recon_days_off[\"day\"] = pd.to_datetime(recon_days_off[\"day\"])\n",
        "    spresso_recon = pd.merge(\n",
        "        spresso_recon, recon_days_off, how=\"left\", on=[\"drn\", \"day\"]\n",
        "    )\n",
        "    spresso_recon[\"current_day_off\"].fillna(0, inplace=True)\n",
        "    spresso_recon[\"assigned_day\"] = 1\n",
        "    # changing day off if drivers completed more than 5 trips\n",
        "    spresso_recon.loc[spresso_recon[\"trips\"] > 5, \"current_day_off\"] = 0\n",
        "    spresso_recon[\"effective_days\"] = spresso_recon[\"assigned_day\"] - (\n",
        "        spresso_recon[\"current_day_off\"] + spresso_recon[\"pick_drop_date\"]\n",
        "    )\n",
        "    spresso_recon[\"effective_days\"] = np.maximum(spresso_recon[\"effective_days\"], 0)\n",
        "\n",
        "    dayoff_unassigned = recon_days_off[\n",
        "        ~recon_days_off[\"drn\"].isin(spresso_recon[\"drn\"])\n",
        "    ]\n",
        "    if len(dayoff_unassigned) > 0:\n",
        "        col = [\"drn\"]\n",
        "        dayoff_unassigned_list = dayoff_unassigned[col]\n",
        "        dayoff_unassigned_list\n",
        "        table_string = slack_texttable(dayoff_unassigned_list, [\"t\"])\n",
        "        post_to_slack(\n",
        "            details=table_string,\n",
        "            channel=slack_channel,\n",
        "            title=\"day off for unassigned DRN!\",\n",
        "        )\n",
        "\n",
        "    high_supply_hour = perf_df[perf_df[\"supply_hours\"] >= 24]\n",
        "    if len(high_supply_hour) > 0:\n",
        "        col = [\"uber_id\"]\n",
        "        high_supply_hour_list = high_supply_hour[col]\n",
        "        table_string = slack_texttable(high_supply_hour_list, [\"t\"])\n",
        "        post_to_slack(\n",
        "            details=table_string,\n",
        "            channel=slack_channel,\n",
        "            title=\"Uber id exceed 24 hours in uber files!\",\n",
        "        )\n",
        "\n",
        "    spresso_recon = spresso_recon.drop(columns=[\"driver_name_perf\", \"col_to_drop\"])\n",
        "    spresso_recon[\"trips\"].replace(\"\", 0, inplace=True)\n",
        "    spresso_recon[\"trips\"].fillna(0, inplace=True)\n",
        "    spresso_recon[\"supply_hours\"].replace(\"\", 0, inplace=True)\n",
        "    spresso_recon[\"supply_hours\"].fillna(0, inplace=True)\n",
        "\n",
        "    id_have_trip_off = spresso_recon[\n",
        "        (spresso_recon[\"current_day_off\"] == 1) & (spresso_recon[\"trips\"] > 0)\n",
        "    ]\n",
        "\n",
        "    if len(id_have_trip_off) > 0:\n",
        "        col = [\"drn\"]\n",
        "        id_have_trip_off_list = id_have_trip_off[col]\n",
        "        table_string = slack_texttable(id_have_trip_off_list, [\"t\"])\n",
        "        post_to_slack(\n",
        "            details=table_string,\n",
        "            channel=slack_channel,\n",
        "            title=\"DRN with completed trips but flagged as off day!\",\n",
        "        )\n",
        "\n",
        "    spresso_recon[\"cash_collected\"] = np.where(\n",
        "        spresso_recon[\"cash_collected\"] < 0,\n",
        "        np.abs(spresso_recon[\"cash_collected\"]),\n",
        "        spresso_recon[\"cash_collected\"],\n",
        "    )\n",
        "    spresso_recon[\"va_amount\"] = 0\n",
        "    payouts = redshift(\n",
        "        f\"\"\"select reconciliation_date as day,drn,payable as payouts from nigeria.payout where reconciliation_date = '{yesterday}' and status = 'Approved' and product = 's-presso' \"\"\"\n",
        "    )\n",
        "    payouts[\"day\"] = pd.to_datetime(payouts[\"day\"])\n",
        "    spresso_recon = pd.merge(\n",
        "        spresso_recon,\n",
        "        payouts,\n",
        "        how=\"left\",\n",
        "        left_on=[\"day\", \"drn\"],\n",
        "        right_on=[\"day\", \"drn\"],\n",
        "    )\n",
        "    spresso_recon[\"payouts\"].replace(\"\", 0, inplace=True)\n",
        "    spresso_recon[\"payouts\"].fillna(0, inplace=True)\n",
        "\n",
        "    recon_product = redshift(\n",
        "        \"\"\"select product_name as product,daily_remittance as daily_remittance_charge from nigeria.product \"\"\"\n",
        "    )\n",
        "    spresso_recon = pd.merge(spresso_recon, recon_product, how=\"left\", on=\"product\")\n",
        "    yesterday_varchar = yesterday_uber.strftime(\"%A\")\n",
        "    if yesterday_varchar == \"Sunday\":\n",
        "        spresso_recon[\"daily_remittance\"] = 0\n",
        "    else:\n",
        "        spresso_recon[\"daily_remittance\"] = np.where(\n",
        "            spresso_recon[\"effective_days\"] == 0,\n",
        "            0,\n",
        "            spresso_recon[\"effective_days\"] * spresso_recon[\"daily_remittance_charge\"],\n",
        "        )\n",
        "\n",
        "    logger.info(\"Fetching active Incentive programs\")\n",
        "    incentive_check = redshift(\n",
        "        f\"\"\"select *,\n",
        "    CASE WHEN cast('{yesterday_date}' as date) >= start_date and cast('{yesterday_date}' as date) <= end_date then 1 else 0 end as validation\n",
        "    from nigeria.incentive_programs\n",
        "    where product = 'UberGo DTO 48 [S-Presso]' \"\"\"\n",
        "    )\n",
        "\n",
        "    # Filter the dataframe for care_incentive\n",
        "    care_incentive_df = incentive_check[incentive_check[\"program\"] == \"care_incentive\"]\n",
        "    # Initialize care_incentive to 0\n",
        "    spresso_recon[\"care_incentive\"] = 0\n",
        "    # If validation is set for any entry\n",
        "    if care_incentive_df[\"validation\"].any() == 1:\n",
        "        # If yesterday was Sunday, set care_incentive to 0\n",
        "        if yesterday_varchar == \"Sunday\":\n",
        "            pass  # care_incentive remains 0\n",
        "        else:\n",
        "            applied = False  # Flag to indicate if any incentive has been applied\n",
        "            # Iterate over each row in care_incentive_df and apply the condition\n",
        "            for _, row in care_incentive_df.iterrows():\n",
        "                condition = (\n",
        "                    (spresso_recon[\"supply_hours\"] >= row[\"supply_hours\"])\n",
        "                    & (spresso_recon[\"trips\"] >= row[\"trips\"])\n",
        "                    & (spresso_recon[\"net_earnings\"] >= row[\"net_earnings\"])\n",
        "                )\n",
        "                # If the condition applies to any row and no previous incentive has been applied\n",
        "                if condition.any() and not applied:\n",
        "                    spresso_recon.loc[condition, \"care_incentive\"] = row[\"incentives\"]\n",
        "                    applied = True  # Set the flag to True\n",
        "                    break  # Exit the loop as the first incentive has been applied\n",
        "\n",
        "    fuel_incentive_df = incentive_check[incentive_check[\"program\"] == \"fuel_incentive\"]\n",
        "    spresso_recon[\"fuel_incentive\"] = 0\n",
        "    if fuel_incentive_df[\"validation\"].any() == 1:\n",
        "        if yesterday_varchar == \"Sunday\":\n",
        "            pass\n",
        "        else:\n",
        "            applied = False\n",
        "            for _, row in fuel_incentive_df.iterrows():\n",
        "                condition = (\n",
        "                    (spresso_recon[\"supply_hours\"] >= row[\"supply_hours\"])\n",
        "                    & (spresso_recon[\"trips\"] >= row[\"trips\"])\n",
        "                    & (spresso_recon[\"net_earnings\"] >= row[\"net_earnings\"])\n",
        "                )\n",
        "                if condition.any() and not applied:\n",
        "                    spresso_recon.loc[condition, \"fuel_incentive\"] = row[\"incentives\"]\n",
        "                    applied = True\n",
        "                    break\n",
        "\n",
        "    spresso_recon[\"incentives\"] = (\n",
        "        spresso_recon[\"care_incentive\"] + spresso_recon[\"fuel_incentive\"]\n",
        "    )\n",
        "\n",
        "    logger.info(\"Fetching extra charges\")\n",
        "    prev_ec = redshift(\n",
        "        f\"\"\"with ec as (select drn,sum(total_amount) as total_extra_charges, sum(daily_deduction) as total_daily_charges from nigeria.extra_charge group by 1),\n",
        "    recon as (SELECT drn,sum(extra_charges) as total_deduction_recon\n",
        "    FROM nigeria.reconciliation_spresso where day < '{yesterday}' group by 1),\n",
        "    data as (select ec.drn,ec.total_extra_charges,\n",
        "    coalesce(r.total_deduction_recon,0) as total_deduction_recon,\n",
        "    total_daily_charges,\n",
        "    coalesce((total_extra_charges-total_deduction_recon),0) as deduction_different\n",
        "    from ec\n",
        "    left join recon r on ec.drn = r.drn)\n",
        "    select drn, case when deduction_different <= 0 then 0\n",
        "    when deduction_different > 0 and deduction_different > total_daily_charges then total_daily_charges\n",
        "    when deduction_different > 0 and deduction_different <= total_daily_charges then deduction_different\n",
        "    else 0 end as extra_charges from data  \"\"\"\n",
        "    )\n",
        "\n",
        "    spresso_recon = pd.merge(spresso_recon, prev_ec, how=\"left\", on=\"drn\")\n",
        "    spresso_recon[\"extra_charges\"].replace(\"\", 0, inplace=True)\n",
        "    spresso_recon[\"extra_charges\"].fillna(0, inplace=True)\n",
        "    spresso_recon.loc[spresso_recon[\"trips\"] == 0, \"extra_charges\"] = 0\n",
        "\n",
        "    logger.info(\"Fetching prev outstanding\")\n",
        "    prev_outstanding = redshift(\n",
        "        f\"\"\"select drn,sum(outstanding) as prev_outstanding\n",
        "    from nigeria.reconciliation_spresso\n",
        "    where day < '{yesterday}'\n",
        "    group by 1\"\"\"\n",
        "    )\n",
        "    spresso_recon = pd.merge(spresso_recon, prev_outstanding, how=\"left\", on=\"drn\")\n",
        "    spresso_recon[\"prev_outstanding\"].replace(\"\", 0, inplace=True)\n",
        "    spresso_recon[\"prev_outstanding\"].fillna(0, inplace=True)\n",
        "\n",
        "    logger.info(\"Fetching Adjustments\")\n",
        "    recon_adjustments = redshift(\n",
        "        \"\"\"with adjs as (select * from nigeria.adjustments where product = 'UberGo DTO 48 [S-Presso]'),\n",
        "    amount as (select reconciliation_date,drn,sum(adjustment) as adjustment_amount from adjs group by 1,2),\n",
        "    adj_desc as (select * from\n",
        "    (select reconciliation_date,drn,adjustment_description,adjustment_type,row_number() over (partition by reconciliation_date,drn order by reconciliation_date asc) as rnk from adjs) as a\n",
        "    where rnk = 1)\n",
        "    select a.reconciliation_date as day,a.drn,a.adjustment_amount,ad.adjustment_description ,ad.adjustment_type from amount a\n",
        "    left join adj_desc ad on a.reconciliation_date = ad.reconciliation_date and a.drn = ad.drn\"\"\"\n",
        "    )\n",
        "    recon_adjustments[\"day\"] = pd.to_datetime(recon_adjustments[\"day\"])\n",
        "\n",
        "    spresso_recon = pd.merge(\n",
        "        spresso_recon,\n",
        "        recon_adjustments,\n",
        "        how=\"left\",\n",
        "        left_on=[\"day\", \"drn\"],\n",
        "        right_on=[\"day\", \"drn\"],\n",
        "    )\n",
        "    spresso_recon[\"adjustment_amount\"].fillna(0, inplace=True)\n",
        "    spresso_recon[\"adjustment_description\"].fillna(\"\", inplace=True)\n",
        "    spresso_recon[\"adjustment_type\"].fillna(\"\", inplace=True)\n",
        "\n",
        "    logger.info(\"Calculating rest headers\")\n",
        "    spresso_recon[\"moove_balance\"] = (\n",
        "        spresso_recon[\"uber_balance\"]\n",
        "        + spresso_recon[\"va_amount\"]\n",
        "        + spresso_recon[\"bank_statement\"]\n",
        "        + spresso_recon[\"incentives\"]\n",
        "    ) - spresso_recon[\"payouts\"]\n",
        "    spresso_recon[\"outstanding\"] = (\n",
        "        spresso_recon[\"daily_remittance\"]\n",
        "        + spresso_recon[\"extra_charges\"]\n",
        "        + spresso_recon[\"adjustment_amount\"]\n",
        "    ) - spresso_recon[\"moove_balance\"]\n",
        "    spresso_recon[\"amount_due\"] = (\n",
        "        spresso_recon[\"daily_remittance\"]\n",
        "        + spresso_recon[\"extra_charges\"]\n",
        "        + spresso_recon[\"prev_outstanding\"]\n",
        "        + spresso_recon[\"adjustment_amount\"]\n",
        "    )\n",
        "    spresso_recon[\"cum_outstanding\"] = (\n",
        "        spresso_recon[\"amount_due\"] - spresso_recon[\"moove_balance\"]\n",
        "    )\n",
        "    spresso_recon[\"payables\"] = abs(\n",
        "        spresso_recon[\"cum_outstanding\"].apply(lambda x: np.where(x < -500, x, 0))\n",
        "    )\n",
        "    spresso_recon[\"partner_payment\"] = 0\n",
        "    spresso_recon[\"month\"] = spresso_recon[\"day\"].dt.strftime(\"%m\")\n",
        "    spresso_recon[\"year\"] = spresso_recon[\"day\"].dt.strftime(\"%Y\")\n",
        "    current_time = datetime.now()\n",
        "    spresso_recon[\"updated_at\"] = current_time\n",
        "    spresso_recon[\"current_day_off\"] = spresso_recon[\"current_day_off\"].astype(int)\n",
        "    spresso_recon[\"effective_days\"] = spresso_recon[\"effective_days\"].astype(int)\n",
        "    spresso_recon[\"trips\"] = spresso_recon[\"trips\"].astype(int)\n",
        "\n",
        "    logger.info(\"Ordering headers\")\n",
        "    recon_cols = [\n",
        "        \"week\",\n",
        "        \"day\",\n",
        "        \"drn\",\n",
        "        \"uber_name\",\n",
        "        \"uber_id\",\n",
        "        \"product\",\n",
        "        \"vehicle_type\",\n",
        "        \"assigned_day\",\n",
        "        \"current_day_off\",\n",
        "        \"pick_drop_date\",\n",
        "        \"effective_days\",\n",
        "        \"trips\",\n",
        "        \"supply_hours\",\n",
        "        \"cancellation_rate\",\n",
        "        \"net_earnings\",\n",
        "        \"cash_collected\",\n",
        "        \"uber_balance\",\n",
        "        \"va_amount\",\n",
        "        \"bank_statement\",\n",
        "        \"incentives\",\n",
        "        \"payouts\",\n",
        "        \"moove_balance\",\n",
        "        \"daily_remittance\",\n",
        "        \"extra_charges\",\n",
        "        \"prev_outstanding\",\n",
        "        \"amount_due\",\n",
        "        \"outstanding\",\n",
        "        \"cum_outstanding\",\n",
        "        \"payables\",\n",
        "        \"partner_payment\",\n",
        "        \"uber_incentive\",\n",
        "        \"adjustment_amount\",\n",
        "        \"fuel_incentive\",\n",
        "        \"care_incentive\",\n",
        "        \"month\",\n",
        "        \"year\",\n",
        "        \"updated_at\",\n",
        "        \"adjustment_description\",\n",
        "        \"adjustment_type\",\n",
        "    ]\n",
        "    spresso_recon = spresso_recon[recon_cols]\n",
        "\n",
        "    logger.info(\"Inserting payable records\")\n",
        "    payable_list = spresso_recon[spresso_recon[\"payables\"] > 0]\n",
        "    payable_list = payable_list[[\"day\", \"drn\", \"payables\", \"product\"]]\n",
        "    payable_list[\"status\"] = \"Rejected\"\n",
        "    payable_list[\"product\"] = \"s-presso\"\n",
        "    col = {\"day\": \"reconciliation_date\", \"payables\": \"payable\"}\n",
        "    payable_list = payable_list.rename(columns=col)\n",
        "    bulk_insert(\"payout\", payable_list, \"nigeria\")\n",
        "\n",
        "    logger.info(\"Insert payable to sheet for ops team\")\n",
        "    payout_to_sheet = redshift(\n",
        "        \"\"\"select * from nigeria.payout where product = 's-presso' order by reconciliation_date asc \"\"\"\n",
        "    )\n",
        "    payout_to_sheet[\"reconciliation_date\"] = pd.to_datetime(\n",
        "        payout_to_sheet[\"reconciliation_date\"], errors=\"coerce\"\n",
        "    )\n",
        "    payout_to_sheet[\"reconciliation_date\"] = payout_to_sheet[\n",
        "        \"reconciliation_date\"\n",
        "    ].dt.strftime(\"%d/%m/%Y\")\n",
        "    moove = []\n",
        "    sh = gsheet_service.open_by_key(\"1Z28QYhmQv2aAFiwBHq34v4HksqJE2TXaRqRIUwG8JWk\")\n",
        "    moove = sh.worksheet(\"payout\")\n",
        "    num_rows = len(moove.get_all_values())\n",
        "    moove.update(\n",
        "        \"A2:ZZ\", [[\"\" for i in range(moove.col_count)] for j in range(num_rows - 1)]\n",
        "    )\n",
        "    new_data = payout_to_sheet.values.tolist()\n",
        "    moove.update(\"A2\", new_data)\n",
        "\n",
        "    logger.info(\"Inserting Recon records into db\")\n",
        "    bulk_insert(\"reconciliation_spresso\", spresso_recon, \"nigeria\")\n",
        "\n",
        "    logger.info(\"Sending recon records on slack channel\")\n",
        "    yesterday_datetime = datetime.strptime(yesterday_date, \"%Y-%m-%d\")\n",
        "    weekdate_string = yesterday_datetime.strftime(\"%b %d\")\n",
        "    data_to_slack = redshift(\n",
        "        f\"\"\"select * from nigeria.reconciliation_spresso where day >= now() - interval '7' day order by day desc\"\"\"\n",
        "    )\n",
        "    file_to_slack(\n",
        "        data_to_slack,\n",
        "        slack_channel,\n",
        "        \"Daily S-Presso Recon {}.xlsx\".format(weekdate_string),\n",
        "    )\n",
        "\n",
        "    output_data = {}\n",
        "    if \"date\" in event:\n",
        "        output_data[\"date\"] = event[\"date\"]\n",
        "\n",
        "    return output_data\n"
      ]
    }
  ]
}